# ðŸ“š Domain-Specific Q&A Bot using RAG

A Retrieval-Augmented Generation (RAG) chatbot for domain-specific question answering (Legal, Medical, Financial). It uses PDF ingestion, vector search (FAISS), and LLM-based contextual answering with LLaMA-3.

---

## ðŸ§  Tech Stack

- **LLM**: LLaMA-3 via Ollama
- **Embeddings**: `Olla`
- **Vector DB**: FAISS
- **RAG Framework**: Custom with LangChain-style logic
- **Frontend**: Streamlit

---

## ðŸš€ Features

- âœ… Upload and ingest domain-specific PDFs
- âœ… Chunk and embed documents into a vector store
- âœ… Query system with semantic retrieval (top-k)
- âœ… Context-aware answers from LLMs
- âœ… Transparent UI with context display
- âœ… Logs latency, retrieval accuracy, and token usage

---

## ðŸ“‚ Project Structure

```bash
rag-domain-assistant/
â”œâ”€â”€ data/                  # Uploaded and processed PDFs
â”œâ”€â”€ embeddings/            # Embedding generation & FAISS storage
â”œâ”€â”€ faiss_index            # FAISS based vector store
â”œâ”€â”€ llm/                   # LLM interface and prompt handling
â”œâ”€â”€ retrieval/             # Retrieval logic for RAG
â”œâ”€â”€ utils/                 # Chunking, PDF loading, metrics
â”œâ”€â”€ run_pipeline.py        # CLI tool to process and embed documents
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # Project documentation
